# Machine learning

**Обучение** – приобретение необходимой функциональности посредством опыта

**Обучение на примерах**:
- Учимся ходить: Делаем шаг – получилось / нет
- Учим названия животных: показывают и запоминаем

**Машинное обучение** – процесс, в результате которого машина способна показывать 
поведение, которое не было в неё явно запрограммировано 

## Обучение с учителем 
Supervised – Обучение с размеченными данными / метриками

Примеры объектов $x \in X$:
- пациенты
- клиенты банка
- месторождения
- описания проектов
- описания недвижимости

Примеры меток $y \in Y$:
- {здоров, грипп, ангина, ковид}
- {мошенник, честный}
- {есть золота, нет золота}
- доход от реализации
- стоимость

Есть неизвестная целевая зависимость $y(x): X \rightarrow Y $

## Задача обучения с учителем
**Дано:**

**Обучающая выборка**  конечное множество объектов с известными метками 
$$X_{train} = \{(x_1, y_1), ..., (x_m, y_m)\}$$
$$y(x_1) = y_1, ..., y(x_m) = y_m

**Найти**:
**Алгоритм** – способ оценивать метки на новых объектах
$$x \rightarrow blackbox \rightarrow \hat{y}$$


## Цели
1. **Восстановление целевой зависимости**
Уметь восстанавливать метки новых объектов $y(x) – найти зависимость целевой переменной от остальных
2. Интерпретация
Как устроена $y(x)$
3. Оценка качества полученного решения
Например, на сколько ошибаемся в среднем, что ждать при использовании нашего прогноза

## Типы задач обучения с учителем
- Классификация
  - Бинарная. $Y = \{0, 1\}$ или $Y = \{-1, +1\}$ 
  - Скоринговая бинарная $a(x) \in [0, 1]$ 
  - На k непересекающихся классов $Y = \{1, 2, ..., k\}$
  - На k пересекающихся классов $Y = \{0 ,1\}^k$
- Регрессия $Y = \mathbb{R}$
  - Многомерная регрессия $Y = \mathbb{R^n}$
- Прогнозирование (Forecasting)

$X_{train} = \{(x_1, t_1, y_1), ..., (x_m, t_m, y_m)\}$,
$t_1 \leq t2 \leq ...  \leq t_m$

## Пространство объектов
Практически какое угодно:
- медицинские истории
- тексты
- сигналы / временные ряды / последовательности изображения
- векторы / множества 

Для удобства-простоты-теории-практики $X = \mathbb{R^n}$ n-мерное пространство

$x_i = (x_{i1}, ..., x_{in})$ - объект в признаковом описании

$x_{ij}$ - $j$-ый признак

## Целевой признак (target)
**Задача классификации** – целевой признак категориальный

**Задача регрессии** – целевой признак вещественный

Вообще говоря, целевой признак тоже может быть любым, например, графом

> **Замечание:** целевой признак часто условный, приходится самому его формировать


## Генерация признаков
Объект может быть не задан в признаковом пространстве или задан в "плохом"
признаковом пространстве $\rightarrow$ извлечение признаков:

$X \rightarrow \mathbb{R^n}$. Может быть производится автоматически,
чем лучше генерация признаков, тем более простое ML нужно

**Пример:** dag@pk.tinkoff.ru
- Длина = 3
- Доменов = 3
- "1 уровень=ru" = 1
- "1 уровень=com" = 0
- "1 уровень=org" = 0

## Что значит "восстановление целевой зависимости" (меток)
Строим "алгоритм" (гипотезу) $a(x)$, который выдаёт предполагаемые метки

**Формализация качества:**

$L(y, a)$ - функция ошибки (error / loss function)

**Ошибка на объекте $x$**

$L(y(x), a(x))$

$a(x)$ – ответ нашего алгоритмы

> Пример 1: В задаче регрессии $L(y, a) = |y - a|$


Если объекты имеют вероятностную природу, то

$\int_{X \times Y}L(y, a(x))\partial P(x, y) \rightarrow min$

Обучающая выборка или "обучение" (не путать с процессом)
$$X_{train} = \{(x_1, y_1), ..., (x_m, y_m)\}$$

Ошибка на выборке (один из вариантов)
$$ L(a, X_{train}) = \frac{1}{m} \sum^{m}_{i = 1} L(y(x_i), a(x_i)) \rightarrow argmin()$$

## Как минимизируется ошибка?
Минимизация производится в рамках модели

**Модель** – параметрическое семейство алгоритмов $A = \{a(x;w)\}

> $A = \{a(x;w) = w^T x: \mathbb(R^n) \rightarrow \mathbb(R)\}_{w \in \mathbb(R^n)}$


**Обучение – определение параметров алгоритма**, как правило, производится
с помощью оптимизации значения функции ошибки (функционала качества), или её модификации на обучающей выборке

По сути, интеллектуальный перебор алгоритмов... Как – дальше!

## Обобщающая способность
Какое качество (ошибка) алгоритма на новых данных? 
$$L(a, X_{train} \vee L(a, X_{test}))$$
Ошибка на тестовой выборке (Generalization Error/ Test Error) 

обучение $\neq$ запоминание

## Что такое алгоритм?
Мы понимаем под этим функцию $a(x): X \rightarrow Y

## Почему машинное обучение не оптимизация
1. Не знаем меру в $\int_{X \times Y}L(y, a(x))\partial P(x, y) \rightarrow min$
То есть решаем "неправильную задачу оптимизации" и правильный выбор неправильности
– особое умение (регуляризация, проблемно-ориентирование модели и т.п.)
2. **Оптимизация не в классе функций, а в классе алгоритмов** дополнительные требования на решение
3. **Есть контекст**, поэтому много неоптимизационных приёмов, например, аугментация 